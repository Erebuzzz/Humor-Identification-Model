# app.py
import streamlit as st
import pickle
import numpy as np
import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load models and preprocessors
MODEL_SAVE_DIR = "saved_models_new"
with open(f"{MODEL_SAVE_DIR}/preprocessors.pkl", 'rb') as f:
    scaler, variance_selector = pickle.load(f)

# Load the models
models = {}
model_names = ['svm', 'naive_bayes', 'mlp_adam', 'mlp_rmsprop', 'stacking_ensemble']

for name in model_names:
    with open(f"{MODEL_SAVE_DIR}/{name}_model.pkl", 'rb') as f:
        models[name] = pickle.load(f)

# Load BERT model
BERT_LOCAL_PATH = "bert-base-uncased"
bert_model = BertForSequenceClassification.from_pretrained(BERT_LOCAL_PATH, num_labels=2)
bert_model.load_state_dict(torch.load(f"{MODEL_SAVE_DIR}/bert_best_model.pth"))
bert_model.eval()
tokenizer = BertTokenizer.from_pretrained(BERT_LOCAL_PATH)

# Streamlit app layout
st.title("Humor Identification Model")
st.markdown("### Enter your text below to check if it's humorous!")

# Text input
user_input = st.text_area("Input Text", height=150)

# Model selection
selected_model = st.selectbox("Select Model", model_names)

# Prediction button
if st.button("Predict"):
    if user_input:
        # Preprocess input
        tokens = user_input.split()  # Simple tokenization
        features = extract_features_enhanced(tokens, user_input)  # Use your feature extraction function
        features = scaler.transform([features])
        features = variance_selector.transform(features)

        # Get predictions
        if selected_model in models:
            model = models[selected_model]
            if hasattr(model, "predict_proba"):
                prediction = model.predict_proba(features)
                humor_prob = prediction[0][1]
                humor_label = "Humorous" if humor_prob > 0.5 else "Not Humorous"
                st.success(f"Prediction: {humor_label} (Confidence: {humor_prob:.2f})")
            else:
                st.error("Model does not support probability predictions.")
        
        # BERT prediction
        if selected_model == 'bert':
            inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding=True, max_length=256)
            with torch.no_grad():
                outputs = bert_model(**inputs)
                probs = torch.softmax(outputs.logits, dim=1)
                humor_prob = probs[0][1].item()
                humor_label = "Humorous" if humor_prob > 0.5 else "Not Humorous"
                st.success(f"Prediction: {humor_label} (Confidence: {humor_prob:.2f})")
    else:
        st.warning("Please enter some text to analyze.")

# Additional features section
st.markdown("### Additional Features")
st.markdown("You can explore the model's performance and visualizations here.")

# Placeholder for visualizations
if st.button("Show Visualizations"):
    st.markdown("Visualizations will be displayed here.")
    # You can add your visualization code here, e.g., using matplotlib or seaborn

# Run the app
if __name__ == "__main__":
    st.run()